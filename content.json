{"meta":{"title":"鸦鸦的笔记本","subtitle":"想学会走路，必须先学会跑步","description":"本站记录了我在学习工作生活中遇到的问题和思考。","author":"Raven","url":"https://lzx2005.github.io"},"pages":[{"title":"关于本网站作者相关信息","date":"2018-11-30T09:47:03.000Z","updated":"2019-09-10T07:28:52.471Z","comments":true,"path":"about/index.html","permalink":"https://lzx2005.github.io/about/index.html","excerpt":"","text":"大家好，你们可以叫我鸦哥，94年生，喜欢程序开发，目前在杭州工作，从事Java研发。 但是不仅仅是Java，我还喜欢大数据，Python，移动端(Android、iOS)开发，小程序也有我的作品。如果你有好的想法，但是缺一名优质的程序员，可以与我联系。 邮箱：lizhengxian2005@gmail.com 个人主页：lzx2005.github.io Github主页：https://github.com/lzx2005 工作经历 一、泰然金融 2017年2月 至 2019年4月 二、有数金服(实习) 2016年9月 至 2017年1月 三、杭州阳明信息科技有限公司(实习) 2015年10月 至 2016年6月 期刊论文 王博, 叶岩明, 李正先, 等. 基于 LBS 的电子导游系统设计与实现[J]. 电脑编程技巧与维护, 2015 (11): 21-22. 技能清单 Java开发：Java基本技术、Spring、MyBatis、Jfinal 前端开发：Javascript、Bootstrap、Vue、iView、ElementUI、Echarts 移动端开发：Android开发（可以单独开发）、iOS开发（略知一二） Python开发：简单脚本开发、itchat开发 数据库相关：MySQL、MongoDB、Elasticsearch、HBase、H2等 服务器：CentOS 7.0 其他开发：微信公众号开发、微信小程序开发 其他工具：Jenkins、禅道、Wiki、Xshell等 感谢您花时间阅读我的简历，期待能有机会和您共事。 如果您正在招聘，可以发邮件至lizhengxian2005@gmail.com或者crow2005@vip.qq.com联系我获取我的完整简历。请附带上公司全称以及职位要求介绍。"},{"title":"分类","date":"2018-11-26T08:17:08.000Z","updated":"2019-09-10T07:28:52.471Z","comments":false,"path":"categories/index.html","permalink":"https://lzx2005.github.io/categories/index.html","excerpt":"","text":"type: \"categories\""},{"title":"标签","date":"2018-11-26T08:16:56.000Z","updated":"2019-09-10T07:28:52.471Z","comments":false,"path":"tags/index.html","permalink":"https://lzx2005.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"高性能内存队列Disruptor初探","slug":"高性能内存队列Disruptor初探","date":"2019-10-08T07:12:57.000Z","updated":"2019-10-08T07:21:40.002Z","comments":true,"path":"2019/10/08/高性能内存队列Disruptor初探/","link":"","permalink":"https://lzx2005.github.io/2019/10/08/高性能内存队列Disruptor初探/","excerpt":"","text":"这段时间在学习Disruptor的使用，由于其使用略微复杂，所以记录一下防止忘记： 一、定义数据结构 这个数据可以是任何你需要用到的数据，作为一个类封装起来： 12345@Datapublic class OrderEvent &#123; private Long id; private String name;&#125; 二、定义工厂类 工厂类负责提供事件对象： 123456import com.lmax.disruptor.EventFactory;public class OrderEventFactory implements EventFactory&lt;OrderEvent&gt; &#123; public OrderEvent newInstance() &#123; return new OrderEvent(); &#125;&#125; 三、定义消息Handler 消息handler负责处理从队列中弹出的消息。 123456import com.lmax.disruptor.EventHandler;public class OrderEventHandler implements EventHandler&lt;OrderEvent&gt; &#123; public void onEvent(OrderEvent orderEvent, long sequence, boolean endOfBatch) throws Exception &#123; System.out.printf(\"Handler : %d, %s, sequence: %d, endOfBatch: %b \\n\", orderEvent.getId(), orderEvent.getName(), sequence, endOfBatch); &#125;&#125; 四、创建disruptor对象 1234567891011121314OrderEventFactory orderEventFactory = new OrderEventFactory(); // 工厂对象int ringBufferSize = 1024 * 1024; // disruptor环大小ThreadFactory threadFactory = Executors.defaultThreadFactory(); // 线程工厂，使用默认BlockingWaitStrategy blockingWaitStrategy = new BlockingWaitStrategy(); // 等待策略Disruptor&lt;OrderEvent&gt; orderEventDisruptor = new Disruptor&lt;OrderEvent&gt;( orderEventFactory, ringBufferSize, threadFactory, ProducerType.SINGLE, blockingWaitStrategy);OrderEventHandler orderEventHandler = new OrderEventHandler(); // 消息处理handler对象orderEventDisruptor.handleEventsWith(orderEventHandler); // 绑定消息处理handlerorderEventDisruptor.start(); // 启动消息处理 五、发送消息 123456789101112RingBuffer&lt;OrderEvent&gt; ringBuffer = orderEventDisruptor.getRingBuffer(); // 获取环for (long i = 0; i &lt; 100L; i++) &#123; long sequence = ringBuffer.next(); // 得到坐标sequence try &#123; OrderEvent orderEvent = ringBuffer.get(sequence); // 获得消息对象 orderEvent.setId(i); // 填充消息对象参数 orderEvent.setName(\"Event:\" + i); // 填充消息对象参数 System.out.printf(\"publish:%d\\n\", i); &#125; finally &#123; ringBuffer.publish(sequence); // 发布消息 &#125;&#125; 六、总结 通过以上几步即可发送消息，disruptor是一个性能非常高的内存队列，值得深入研究。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"}]},{"title":"扩展Flink使其可以接收RocketMQ消息","slug":"扩展Flink使其可以接收RocketMQ消息","date":"2019-06-11T08:58:22.000Z","updated":"2019-09-10T07:28:52.456Z","comments":true,"path":"2019/06/11/扩展Flink使其可以接收RocketMQ消息/","link":"","permalink":"https://lzx2005.github.io/2019/06/11/扩展Flink使其可以接收RocketMQ消息/","excerpt":"","text":"从官方文档可以看到，Flink支持的数据源有如下几个： Apache Kafka (source/sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Twitter Streaming API (source) 对于其他的源，它也提供了接口给我们实现，扩展性非常好，今天我们就实现一个从RocketMQ取数据的实现。 打开项目，转到Flink处理类，可以看到： 1234567891011object FlinkKafkaConsumerDemo &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val properties = new Properties() properties.setProperty(\"bootstrap.servers\", \"localhost:9092\") properties.setProperty(\"group.id\", \"test\") val data = env.addSource(new FlinkKafkaConsumer[String](\"lzxtest\", new SimpleStringSchema(), properties)) data.print() env.execute(\"FlinkKafkaConsumerDemo\") &#125;&#125; 我们按住Command，点击FlinkKafkaConsumer查看其结构： 12345678910111213141516171819202122232425262728293031public class FlinkKafkaConsumer&lt;T&gt; extends FlinkKafkaConsumerBase&lt;T&gt;&#123;&#125;public abstract class FlinkKafkaConsumerBase&lt;T&gt; extends RichParallelSourceFunction&lt;T&gt; implements CheckpointListener, ResultTypeQueryable&lt;T&gt;, CheckpointedFunction &#123;&#125;public abstract class RichParallelSourceFunction&lt;OUT&gt; extends AbstractRichFunction implements ParallelSourceFunction&lt;OUT&gt; &#123;&#125;public interface ParallelSourceFunction&lt;OUT&gt; extends SourceFunction&lt;OUT&gt; &#123;&#125;public interface SourceFunction&lt;T&gt; extends Function, Serializable &#123; void run(SourceFunction.SourceContext&lt;T&gt; var1) throws Exception; void cancel(); @Public public interface SourceContext&lt;T&gt; &#123; void collect(T var1); @PublicEvolving void collectWithTimestamp(T var1, long var2); @PublicEvolving void emitWatermark(Watermark var1); @PublicEvolving void markAsTemporarilyIdle(); Object getCheckpointLock(); void close(); &#125;&#125; 可以看到，要实现自定义的Source，只需要实现接口SourceFunction即可，如果需要并行消费，可以实现ParallelSourceFunction。 创建类RocketMQSourceFunction，继承SourceFunction： 1234567891011public class RocketMQSourceFunction implements SourceFunction&lt;String&gt; &#123; @Override public void run(SourceContext&lt;String&gt; sourceContext) throws Exception &#123; // todo &#125; @Override public void cancel() &#123; // todo &#125;&#125; 首先我们需要准备一个RocketMQ的消费者客户端，打开pom.xml，添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.3.0&lt;/version&gt; &lt;!-- 版本号注意修改 --&gt;&lt;/dependency&gt; 对于RocketMQSourceFunction来说，我们需要初始化一个Consumer，所以添加代码如下： 1234public class RocketMQSourceFunction implements SourceFunction&lt;String&gt; &#123; private static final DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"consumerGroupTest\"); ...&#125; 这样，当类在加载的时候，系统会创建一个consumer。对于一个Consumer来说，还需要知道我们要消费的nameSrvAddr和Topic是什么，所以我们添加字段： 123456789public class RocketMQSourceFunction implements SourceFunction&lt;String&gt; &#123; public RocketMQSourceFunction(String nameSrvAddr, String topic) &#123; this.nameSrvAddr = nameSrvAddr; this.topic = topic; &#125; private String nameSrvAddr; private String topic; ...&#125; 重写Run方法： 12345678910111213@Overridepublic void run(SourceContext&lt;String&gt; sourceContext) throws Exception &#123; consumer.setNamesrvAddr(nameSrvAddr); consumer.subscribe(topic, \"*\"); consumer.registerMessageListener((MessageListenerConcurrently) (msgs, context) -&gt; &#123; msgs.forEach(msg -&gt; &#123; sourceContext.collect(new String(msg.getBody(), Charset.forName(\"UTF-8\"))); &#125;); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;); consumer.start(); System.out.println(\"consumer started\");&#125; consumer会在接收到消息时，发送消息到sourceContext中，这样Flink的流就可以接收到消息了。同时不要忘了重写cancal方法： 1234@Overridepublic void cancel() &#123; consumer.shutdown();&#125; 这样，一个完整的RocketMQ的数据源接收器我们已经实现好了，在需要用到的Flink代码中加入： 12345678object FlinkRocketMQConsumerDemo &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val rmq = env.addSource(new RocketMQSourceFunctionJava(\"localhost:9876\", \"lzxtest\")).setParallelism(1) rmq.print().setParallelism(1) env.execute(\"FlinkRocketMQConsumerDemo\") &#125;&#125; 这样，Flink即可接收到RocketMQ的消息了。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"},{"name":"大数据","slug":"大数据","permalink":"https://lzx2005.github.io/tags/大数据/"},{"name":"Flink","slug":"Flink","permalink":"https://lzx2005.github.io/tags/Flink/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"https://lzx2005.github.io/tags/RocketMQ/"}]},{"title":"ThoughtWorks的面试题:Conference Track Management解题思路","slug":"ThoughtWorks的面试题-Conference-Track-Management解题思路","date":"2019-05-28T09:33:53.000Z","updated":"2019-09-10T07:28:52.430Z","comments":true,"path":"2019/05/28/ThoughtWorks的面试题-Conference-Track-Management解题思路/","link":"","permalink":"https://lzx2005.github.io/2019/05/28/ThoughtWorks的面试题-Conference-Track-Management解题思路/","excerpt":"","text":"会议排期管理 题目 最近看到一个ThoughtWorks的面试题，这边实现一下，问题如下： Problem Two: Conference Track Management You are planning a big programming conference and have received many proposals which have passed the initial screen process but you're having trouble fitting them into the time constraints of the day -- there are so many possibilities! So you write a program to do it for you. The conference has multiple tracks each of which has a morning and afternoon session. Each session contains multiple talks. Morning sessions begin at 9am and must finish before 12 noon, for lunch. Afternoon sessions begin at 1pm and must finish in time for the networking event. The networking event can start no earlier than 4:00 and no later than 5:00. No talk title has numbers in it. All talk lengths are either in minutes (not hours) or lightning (5 minutes). Presenters will be very punctual; there needs to be no gap between sessions. Note that depending on how you choose to complete this problem, your solution may give a different ordering or combination of talks into tracks. This is acceptable; you don’t need to exactly duplicate the sample output given here. Test input: 12345678910111213141516171819Writing Fast Tests Against Enterprise Rails 60minOverdoing it in Python 45minLua for the Masses 30minRuby Errors from Mismatched Gem Versions 45minCommon Ruby Errors 45minRails for Python Developers lightningCommunicating Over Distance 60minAccounting-Driven Development 45minWoah 30minSit Down and Write 30minPair Programming vs Noise 45minRails Magic 60minRuby on Rails: Why We Should Move On 60minClojure Ate Scala (on my project) 45minProgramming in the Boondocks of Seattle 30minRuby vs. Clojure for Back-End Development 30minRuby on Rails Legacy App Maintenance 60minA World Without HackerNews 30minUser Interface CSS in Rails Apps 30min Test output: 1234567891011121314151617181920212223242526Track 1:09:00AM Writing Fast Tests Against Enterprise Rails 60min10:00AM Overdoing it in Python 45min10:45AM Lua for the Masses 30min11:15AM Ruby Errors from Mismatched Gem Versions 45min12:00PM Lunch01:00PM Ruby on Rails: Why We Should Move On 60min02:00PM Common Ruby Errors 45min02:45PM Pair Programming vs Noise 45min03:30PM Programming in the Boondocks of Seattle 30min04:00PM Ruby vs. Clojure for Back-End Development 30min04:30PM User Interface CSS in Rails Apps 30min05:00PM Networking EventTrack 2:09:00AM Communicating Over Distance 60min10:00AM Rails Magic 60min11:00AM Woah 30min11:30AM Sit Down and Write 30min12:00PM Lunch01:00PM Accounting-Driven Development 45min01:45PM Clojure Ate Scala (on my project) 45min02:30PM A World Without HackerNews 30min03:00PM Ruby on Rails Legacy App Maintenance 60min04:00PM Rails for Python Developers lightning05:00PM Networking Event 代码介绍 实体类 会议管理系统包含多个实体类 Track : 负责记录每一天的行程，所有的排期，可以包含多个Track，也就是多天。 Session : 负责记录每一天上午，或者下午(可以扩展比如晚上)，记录了总共可以分配的时间，剩余可用的时间，开始时间(早上是9点，下午是1点)以及Talk列表。 Talk : 一次会议，包含的是会议标题，会议耗时。 对于使用者来说，只需要进行一次new Track()操作，即可创建出一个包含上午和下午两个Session的Track，上午可用时间为180分钟，下午可用时间为240分钟，每一个Session生成了一个空的Talk List。 枚举类 SessionType : 定义一个Session，包括Session总共可分配时间以及Session开始时间，使用LocalTime定义时分秒。 总流程类 TrackService : 处理传递过来的Talk列表，分配时间，安排会议，翻译一个Track列表。 工具类 FileUtils : 读取文件每一行，返回一个List&lt;String&gt; 调用流程 读取文件： 1234List&lt;String&gt; list = FileUtils.read(new File(FileLaction.TEXT_FILE));if (list == null) &#123; throw new RuntimeException(\"file is null\");&#125; 生成Talk列表 12345TreeSet&lt;Talk&gt; talks = new TreeSet&lt;&gt;(); //使用TreeSet是可以让列表以时间排序list.forEach(title -&gt; &#123; Talk talk = new Talk(title); talks.add(talk);&#125;); 调用Service 12TrackService trackService = TrackService.getInstance(); //TrackService是单例的List&lt;Track&gt; tracks = trackService.manage(talks); 格式化输出 123456789for (int i = 0; i &lt; tracks.size(); i++) &#123; Track track = tracks.get(i); System.out.println(\"Track \" + (i + 1) + \":\"); System.out.print(track.getMorning().toString()); System.out.println(\"12:00PM Lunch\"); System.out.print(track.getAfternoon().toString()); System.out.println(\"05:00PM Networking Event\"); System.out.println();&#125; 得到结果 1234567891011121314151617181920212223242526Track 1:09:00AM Communicating Over Distance 60min10:00AM Ruby on Rails: Why We Should Move On 60min11:00AM Writing Fast Tests Against Enterprise Rails 60min12:00PM Lunch01:00PM Rails Magic 60min02:00PM Ruby on Rails Legacy App Maintenance 60min03:00PM Overdoing it in Python 45min03:45PM Ruby Errors from Mismatched Gem Versions 45min04:30PM Programming in the Boondocks of Seattle 30min05:00PM Networking EventTrack 2:09:00AM Accounting-Driven Development 45min09:45AM Pair Programming vs Noise 45min10:30AM Clojure Ate Scala (on my project) 45min11:15AM Common Ruby Errors 45min12:00PM Lunch01:00PM Woah 30min01:30PM Ruby vs. Clojure for Back-End Development 30min02:00PM Sit Down and Write 30min02:30PM Lua for the Masses 30min03:00PM User Interface CSS in Rails Apps 30min03:30PM A World Without HackerNews 30min04:00PM Rails for Python Developers lightning05:00PM Networking Event 我靠这份代码拿到了面试机会。","categories":[{"name":"面试","slug":"面试","permalink":"https://lzx2005.github.io/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://lzx2005.github.io/tags/面试/"},{"name":"算法","slug":"算法","permalink":"https://lzx2005.github.io/tags/算法/"}]},{"title":"RocketMQ单机部署远程访问踩到的一个坑","slug":"RocketMQ单机部署远程访问踩到的一个坑","date":"2019-05-23T06:34:26.000Z","updated":"2019-09-10T07:28:52.430Z","comments":true,"path":"2019/05/23/RocketMQ单机部署远程访问踩到的一个坑/","link":"","permalink":"https://lzx2005.github.io/2019/05/23/RocketMQ单机部署远程访问踩到的一个坑/","excerpt":"","text":"之前根据rocketmq.apache.org的get started步骤部署的RocketMQ单节点，在远程访问的时候会出现如下问题： 1234567Startorg.apache.rocketmq.remoting.exception.RemotingTooMuchRequestException: sendDefaultImpl call timeout at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(DefaultMQProducerImpl.java:612) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:1253) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:1203) at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:214) at com.cmit.fabric.java.rocketmq.RocketMQTest.producer.ProducerTest.producerStart(ProducerTest.java:39) at com.cmit.fabric.java.rocketmq.RocketMQTest.producer.ProducerTest.main(ProducerTest.java:27) 在查询了相关资料以后，得到解决方案，假设我们的IP是：172.16.10.53，修改配置文件broker.conf，加上： 1brokerIP1=172.16.10.53 且需要在启动namesrv和broker的时候加上本机IP： 12sh bin/mqnamesrv -n 172.16.10.53:9876sh bin/mqbroker -n 172.16.10.53:9876 -c conf/broker.conf autoCreateTopicEnable=true 启动后远程访问调用成功。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"}]},{"title":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（三）","slug":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（三）","date":"2019-05-23T02:46:10.000Z","updated":"2019-09-10T07:28:52.458Z","comments":true,"path":"2019/05/23/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（三）/","link":"","permalink":"https://lzx2005.github.io/2019/05/23/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（三）/","excerpt":"","text":"上一篇文章我们实现了RocketMQ的输出： 《改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（二）》：快速链接 这篇我们将改造后的代码进行编译和部署，随后进行测试。 打包 otter已经把打包的shell脚本写好了，查看目录docker/build.sh 12345678910#!/bin/bash# 省略不相关命令...else rm -rf $BASE/node.*.tar.gz ; rm -rf $BASE/manager.*.tar.gz ; cd $BASE/../ &amp;&amp; mvn clean package -Dmaven.test.skip -Denv=release &amp;&amp; cd $current_path ; cp $BASE/../target/node.deployer-*.tar.gz $BASE/ cp $BASE/../target/manager.deployer-*.tar.gz $BASE/# docker build --no-cache -t canal/otter-all $BASE/fi 将docker build 的语句注释，执行sh docker.sh即可进行打包，打包完成后会在当前目录下生成manager和node两个tar.gz，我们将其远程传输到服务器中： 12scp manager.deployer-4.2.18-SNAPSHOT.tar.gz root@ip:~/otterscp node.deployer-4.2.18-SNAPSHOT.tar.gz root@ip:~/otter 部署 进入部署服务器，使用tar解压： 12345cd ~/ottermkdir managermkdir nodetar zxvf manager.deployer-4.2.18-SNAPSHOT.tar.gz -C managertar zxvf node.deployer-4.2.18-SNAPSHOT.tar.gz -C node 导入数据库文件 manager运行时需要依赖MySQL保存配置信息，根据文档Manager_Quickstart我们需要下载ddl文件进行数据库部署： 12wget https://raw.github.com/alibaba/otter/master/manager/deployer/src/main/resources/sql/otter-manager-schema.sql mysql -h localhost -uroot -p 执行DDL： 1234mysql &gt; create database otter;mysql &gt; use otter;mysql &gt; source /root/otter/otter-manager-schema.sql;mysql &gt; exit; 部署manager 进入manager目录，修改conf/otter.properties文件，填写必须的参数： 12345678910## otter manager domain nameotter.domainName = 127.0.0.1## otter manager http portotter.port = 8080## otter manager database configotter.database.driver.class.name = com.mysql.jdbc.Driverotter.database.driver.url = jdbc:mysql://127.0.0.1:3306/otterotter.database.driver.username = roototter.database.driver.password = root... 保存后，执行部署 1sh ./bin/startup.sh 打开浏览器，输入ip:8080，查看是否可以看到otter管理后台： 日志可以在manager/logs/manager.log中查看，管理员默认的账号密码为admin:admin，记得修改密码，不然有安全隐患。 配置Zookeeper 打开manager管理后台，以管理员账号登录，点击机器管理-Zookeeper管理，点击添加，输入集群信息，点击保存。这步必须执行，之后部署Node需要用到。 部署Node 打开manager管理后台，以管理员账号登录，点击机器管理-Node管理，点击添加，输入Node相关信息： 点击保存，可以得到保存好的Node节点，且状态为未启动： 复制序号列下的内容，图中为1。打开服务器后台，进入目录node 12cd ~/otter/nodeecho 1 &gt; conf/nid 这样就表示这个Node是为了上图这个Node而配置的，然后启动Node： 1./bin/startup.sh 刷新后台，可以看到Node状态为已启动 到此为止，我们的Manager和Node已经部署完毕。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"}]},{"title":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（二）","slug":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（二）","date":"2019-05-20T07:38:36.000Z","updated":"2019-09-10T07:28:52.464Z","comments":true,"path":"2019/05/20/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（二）/","link":"","permalink":"https://lzx2005.github.io/2019/05/20/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（二）/","excerpt":"","text":"上篇文章我们分析了Otter的代码结构： 《改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（一）》：快速链接 这篇我们开始正式进行改造。 改造清单 定义RocketMQ实体，定义一个RocketMQ的目标源。 改造管理系统的逻辑以及前端，支持RocketMQ的配置。 改造Transerformer模块，支持将数据转换成RocketMQ需要的格式。 改造Load模块，支持将数据向RocketMQ发送。 定义RocketMQ otter 中定义一个数据源是由DataMediaSource决定的。 123456789public class DataMediaSource implements Serializable &#123; private Long id; private String name; private DataMediaType type; private String encode; private Date gmtCreate; private Date gmtModified; // get and set...&#125; 查看这个表的继承类，可以看到DbMediaSource，定义了url、username、password、driver等连接数据库必须要的参数。于是我就照着他的实现方式来进行编写支持RocketMQ的新实现。 RocketMQMediaSource 编写类RocketMQMediaSource，定义连接到RocketMQ的必要参数： 1234567public class RocketMqMediaSource extends DataMediaSource &#123; // 定义连接到RocketMQ的gourpName private String groupName; // 定义连接到RocketMQ的namesrvAddr private String namesrvAddr; // get and set...&#125; 同时，在枚举类DataMediaType添加名称为ROCKETMQ的枚举。 RocketMQDataMedia 实现DataMedia类DataMedia&lt;Source extends DataMediaSource&gt;，泛型定义为RocketMQMediaSource 1public class RocketMqDataMedia extends DataMedia&lt;RocketMQMediaSource&gt; &#123;&#125; 此类定义了一种数据媒介为RocketMQ。 改造管理系统 添加数据源页面 打开addDataSource.vm可以看到添加数据源的前端代码，此代码是由模板引擎velocity编写的，Java会根据指定的格式渲染HTML页面展示给客户端。找到数据源下拉框Select，添加RocketMQ选项。 1234567&lt;td&gt; &lt;select id=\"sourceType\" name=\"$dataMediaSourceGroup.type.key\" onchange=\"changeform();\" &gt; &lt;option value=\"MYSQL\"&gt;MySQL&lt;/option&gt; &lt;option value=\"ORACLE\"&gt;Oracle&lt;/option&gt; &lt;option value=\"ROCKETMQ\"&gt;RocketMQ&lt;/option&gt; &lt;!--添加一行，提供RocketMQ的选择--&gt; &lt;/select&gt;&lt;span class=\"red\"&gt;*&lt;/span&gt;&lt;/td&gt; 对于sourceType这个参数，后端可以识别为枚举类DataMediaType.ROCKETMQ，所以后端可以不需要修改其他的地方。 但是RocketMQ还需要填写额外的参数gourpName，我们需要对前端进行一定的改造，由于作者在这里预留了changeFrom()函数，触发条件为select下拉框被改变的时候，那么我们就可以判断当其改变时，作出相应的表单内容调整。 首先我们定义需要填写的groupName： 12345678&lt;tr id=\"group_name_tr\" style=\"display: none;\"&gt; &lt;!-- 注意第一行，很重要 --&gt; &lt;th&gt;GroupName：&lt;/th&gt; &lt;td&gt; &lt;input id=\"sourceGroupName\" name=\"$dataMediaSourceGroup.groupName.key\" value=\"$!dataMediaSourceGroup.groupName.value\" type=\"text\" class=\"setting_input\"/&gt;&lt;span class=\"red\"&gt;*&lt;/span&gt; &lt;br /&gt; &lt;span class=\"red\"&gt;#addDataSourceMessage ($dataMediaSourceGroup.groupName)&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt; 重写changeForm()函数： 1234567891011121314function changeform() &#123; console.log(\"changed form\") //获取 sourceType 的Dom对象 var sourceType = document.getElementById('sourceType').value; //获取 groupName 的Dom对象 var group_name_tr = document.getElementById(\"group_name_tr\"); if (\"ROCKETMQ\" === sourceType) &#123; // 如果是RocketMQ，则显示 groupName 的填写框 group_name_tr.style.display = \"table-row\"; &#125; else &#123; // 否则不显示 group_name_tr.style.display = \"none\"; &#125;&#125; 这样我们就可以根据下拉框自动展示需要填写的表单了： 验证连接数据源改造 同时，需要将验证连接数据源，这个按钮所请求的后端逻辑进行改造，否则无法通过提交： 根据DEBUG我们得知，该验证逻辑定义在DataSourceChecker类的check()方法中，所以我们对该方法进行改造： 123456789public String check(String url, String username, String password, String encode, String sourceType, String groupName) &#123; if (\"MYSQL\".equalsIgnoreCase(sourceType) || \"ORACLE\".equalsIgnoreCase(sourceType)) &#123; return checkDB(url, username, password, encode, sourceType); &#125; else if (\"RocketMQ\".equalsIgnoreCase(sourceType)) &#123; return checkMQ(url, username, password, encode, sourceType, groupName); &#125; else &#123; return DATABASE_SUCCESS; &#125;&#125; 改造后，系统可以根据入参sourceType执行不同的验证逻辑。对于RocketMQ的验证逻辑目前我只是将连接打开再关闭，如果没有报错则返回成功，但是目前看下来，好像这个逻辑并不严谨，希望讨论获得更好的方案： 12345678910111213private String checkMQ(String url, String username, String password, String encode, String sourceType, String groupName) &#123; DefaultMQProducer defaultMQProducer =new DefaultMQProducerImpl(groupName); defaultMQProducer.setNamesrvAddr(url); try &#123; defaultMQProducer.start(); return DATABASE_SUCCESS; &#125; catch (MQClientException e) &#123; logger.error(\"connect to RocketMQ failed\", e); return DATABASE_FAIL; &#125; finally &#123; defaultMQProducer.shutdown(); &#125;&#125; 实现RocketMQTransformer Otter对从Canal拉取到的数据进行了四步处理，分别是：Select、Extract、Transform、Load。 对于新的输出来说，只需要改造Transformer和Load即可。 查看源代码，找到node模块下的包com.alibaba.otter.node.etl.transform，可以看到基于MySQL和Oracle的实现： 该实现继承自类com.alibaba.otter.node.etl.transform.transformer.OtterTransformer： 123public interface OtterTransformer&lt;S, T&gt; &#123; S transform(T data, OtterTransformerContext context);&#125; 所以我们只需要实现该接口即可： 1234567891011121314151617181920212223242526public class RocketMQTransformer implements OtterTransformer&lt;EventData, EventData&gt; &#123; @Override public EventData transform(EventData data, OtterTransformerContext context) &#123; List&lt;CanalEntry.Column&gt; beforeColumns = data.getBeforeColumns(); List&lt;CanalEntry.Column&gt; afterColumns = data.getAfterColumns(); HashMap&lt;String, Object&gt; before = new HashMap&lt;String, Object&gt;(); HashMap&lt;String, Object&gt; after = new HashMap&lt;String, Object&gt;(); for (CanalEntry.Column column : beforeColumns) &#123; before.put(column.getName(), column.getValue()); &#125; for (CanalEntry.Column column : afterColumns) &#123; after.put(column.getName(), column.getValue()); &#125; data.setAfter(after); data.setBefore(before); data.setAfterColumns(null); data.setBeforeColumns(null); if (data.getKeys().size() &gt; 0) &#123; data.setPrimaryKey(data.getKeys().get(0).getColumnValue()); &#125; return data; &#125;&#125; RocketMQTransformer是OtterTransformer的RocketMQ版本实现，主要是根据使用方的需求进行数据的整理封装，返回合理的数据。这里我们将每个字段的改变前和改变后作为值存入数据集。 实现RocketMQLoader 实现Loader也是相同的，找到Loader接口com.alibaba.otter.node.etl.load.loader.OtterLoader： 123public interface OtterLoader&lt;P, R&gt; &#123; R load(P data);&#125; 进行实现： 1234567891011121314public class RocketMQLoader implements OtterLoader&lt;DbBatch, List&lt;LoadContext&gt;&gt;, BeanFactoryAware &#123; private BeanFactory beanFactory; @Override public List&lt;LoadContext&gt; load(DbBatch data) &#123; // 发送MQ方法省略 return sendToRocketMQ(data); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory = beanFactory; &#125;&#125; 由于篇幅限制，我省略了一些实现，包括管道配置、RocketMQClient工具类编写。 更多代码请参考我Fork出来的github项目：https://github.com/lzx2005/otter，我修改的代码在分支mq-extend-try-1中。 接下来的第三篇文章就是对我们新改造的模块进行各方面的压测，确保其可以继承Otter优秀的性能，同时也能完美做到组件扩展。 未完待续。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"}]},{"title":"慕课网Spark大数据课程笔记-Hadoop2.6.0环境搭建","slug":"慕课网Spark大数据课程笔记-Hadoop2-6-0环境搭建","date":"2019-05-15T11:41:13.000Z","updated":"2019-09-10T07:28:52.455Z","comments":true,"path":"2019/05/15/慕课网Spark大数据课程笔记-Hadoop2-6-0环境搭建/","link":"","permalink":"https://lzx2005.github.io/2019/05/15/慕课网Spark大数据课程笔记-Hadoop2-6-0环境搭建/","excerpt":"","text":"系统准备 新建文件： /home/hadoop software ：存放软件安装包 app : 存放软件目录 data ： 存放测试数据 source ： 存放软件源码：spark 下载Hadoop 下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 下载版本：2.6.0-cdh5.7.0 1wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.0.tar.gz 配置环境和Hadoop 去官网 hadoop.apache.org 查看安装手册 安装JDK 12export JAVA_HOME=[jdk]export PATH=$JAVA_HOME/bin:$PATH 机器参数设置 123456789vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=hadoop001vim /etc/hosts127.0.0.1 localhost[ip] hadoop001 SSH免密码登录 12ssh-keygen -t rsacp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys 修改hadoop-env.sh 12345cd /root/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoopvim hadoop-env.sh修改export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64 修改core-site.xml 12cd /root/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoopvim core-site.xml 添加 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/hadoop/temp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml 12cd /root/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoopvim hdfs-site.xml 添加 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 启动Hadoop 第一次启动：格式化HDFS 1bin/hdfs namenode -format 启动HDFS 1sbin/start-dfs.sh 验证Hadoop 在浏览器中输入IP:50070，如果能访问到Hadoop后端，则启动成功：","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/categories/读书笔记/"},{"name":"慕课网Spark大数据课程","slug":"读书笔记/慕课网Spark大数据课程","permalink":"https://lzx2005.github.io/categories/读书笔记/慕课网Spark大数据课程/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/tags/读书笔记/"},{"name":"大数据","slug":"大数据","permalink":"https://lzx2005.github.io/tags/大数据/"},{"name":"Spark","slug":"Spark","permalink":"https://lzx2005.github.io/tags/Spark/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://lzx2005.github.io/tags/Hadoop/"}]},{"title":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（一）","slug":"改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（一）","date":"2019-05-15T02:27:49.000Z","updated":"2019-09-10T07:28:52.457Z","comments":true,"path":"2019/05/15/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（一）/","link":"","permalink":"https://lzx2005.github.io/2019/05/15/改造阿里巴巴otter框架，使其支持RocketMQ配置和输出（一）/","excerpt":"","text":"最近接到了一个需求，需要同步业务数据库至MQ，再进行相应的操作。 对于这种需求，其实阿里巴巴出品的canal（https://github.com/alibaba/canal）就可以做到，但是领导提出了更变态的需求，需要有前端界面，可以动态配置，读取什么表，发送到哪个MQ，哪个topic，哪个tag，还得需要有监控界面，查看每个时间段同步的数据，对于这种可配置的需求，阿里巴巴otter（https://github.com/alibaba/otter）可以完美做到，可是otter内置的canal阉割了发送到kafka和RocketMQ的功能。再三调研和判断后，我们还是决定对otter进行改造，将阉割了的发送MQ功能，给它装回去。 对我来说，我只有使用otter的经验，对于其源码还是有点生疏，所以我就在自己的博客开了一个新坑，记录一下我改造otter的过程。如果有不好的地方，欢迎砸我回复。 熟悉源码 12git clone https://github.com/alibaba/otter.gitcd otter 下载源码后，我们使用Idea打开项目，引入Maven包，可以看到目录如下： 1234567891011121314151617otter|--manager |--biz |--deployer |--web|--node |--canal |--common |--deployer |--etl |--extend|--shared |--arbitrate |--common |--communication |--etl |--push Manager manager是对node节点进行管理，数据统计，node节点之间的协调，基本信息的同步等。 biz：业务处理内容 deployer：manager打包和启动的类，主要内置一个jetty的服务启动器，通过OtterManagerLauncher启动 web：manager的web页面相关内容，包括接口 Node node是实际上进行数据同步的工程 canal：canal客户端，就是订阅binlog的客户端，Otter采用的是Embed的方式引入Canal common：公共内容定义 deployer：打包工具，运行脚本，内置jetty服务器，启动类OtterLauncher etl：S.E.T.L 调度、处理的实现，是Otter最复杂、也是最核心的部分 extend：留有的扩展类，可以自定义一些处理过程 Shared shared是manager和node公有的子系统 arbitrate：仲裁器相关内容，代码比较多，后面我们慢慢分析 common：manager和node用到的一些公共类 communication：远程调用相关内容 etl push 以上部分来自博文：https://www.cnblogs.com/f-zhao/p/8328987.html 改造方案 根据otter的github主页图，可以很清晰的了解otter的运行模式和结构。 阅读了官方文档：https://github.com/alibaba/otter/wiki/Otter扩展性，我们对otter的扩展性有了一定了解 为了增加MQ的功能，主要还是要对T（transformer）和L（load）进行改造 同时需要对manager进行二次开发，以满足配置MQ的需求。 所以改造的功能图如下所示： 接下来就是对Manager进行改造了。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"},{"name":"中间件","slug":"Java/中间件","permalink":"https://lzx2005.github.io/categories/Java/中间件/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"中间件","slug":"中间件","permalink":"https://lzx2005.github.io/tags/中间件/"}]},{"title":"填充每个节点的下一个右侧节点指针2","slug":"填充每个节点的下一个右侧节点指针2","date":"2019-04-09T01:39:17.000Z","updated":"2019-09-10T07:28:52.434Z","comments":true,"path":"2019/04/09/填充每个节点的下一个右侧节点指针2/","link":"","permalink":"https://lzx2005.github.io/2019/04/09/填充每个节点的下一个右侧节点指针2/","excerpt":"","text":"题目 https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/ 给定一个二叉树 123456class Node &#123; int val; Node left; Node right; Node next;&#125; 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 初始状态下，所有 next 指针都被设置为 NULL。 sss 思路 Solution 二叉树层序遍历，以上图为例： 新建一个Queue，使用层序遍历二叉树的方式，将数据写入队列中。 队列我们使用LinkedList&lt;Node&gt;，并将root直接写入队列 12Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;();queue.add(root); 使用表的形式表示队列： index 0 1 2 4 5 6 7 8 9 Node 1 嵌套第一个循环 123while (!queue.isEmpty()) &#123; // 二叉树层序遍历，每次遍历都会添加每一行新的数据，所以除非遍历完，否则不会空。&#125; 在这个循环内第一步要做的事情是获取队列queue的size 1int size = queue.size(); //获取当前队列中已有的节点的数量 进入第二层循环 12345678910111213141516while (!queue.isEmpty()) &#123; int size = queue.size(); while (size-- &gt; 0)&#123; // 大的循环每一次都是一层，每一层，我们循环将节点的Next指向下一个节点 Node node = queue.remove(); if (size &gt; 0) &#123; node.next = queue.peek(); &#125; if (node.left != null) &#123; queue.add(node.left); &#125; if (node.right != null) &#123; queue.add(node.right); &#125; &#125;&#125; 第一次循环 得到size = 1 size = 1 &gt; 0 进入循环，size 变为0 Node node= queue.remove(); 得到当前队列中的节点 node 判断当前size 不大于0 ，则不进行node.next指向操作 将node不为空的左节点存入队列，将不为空的右节点存入队列 则当前队列内容为： index 0 1 2 4 5 6 7 8 9 Node 2 3 第二次循环 得到size = 2 size = 2 &gt; 0 进入循环，size 变为1 Node node= queue.remove(); 得到当前队列中的节点 node， 因为弹出了第一个节点，当前队列为： index 0 1 Node 3 判断当前size 大于0 ，则进行node.next指向操作 node.next = queue.peek() ，那么2 就 指向了 3，但是3没有弹出。 将node不为空的左节点存入队列，将不为空的右节点存入队列，则当前队列为： index 0 1 2 Node 3 4 5 继续循环，size = 1 &gt; 0 ,进入二层循环，size 变为0 Node node= queue.remove(); 得到当前队列中的节点 node index 0 1 2 Node 4 5 判断当前size 不大于0 ，则不进行node.next指向操作，所以 3 指向的是Null 将3的左右孩子存入队列： index 0 1 2 Node 4 5 7 内层循环结束，开始新的一轮循环，直到所有的节点都指向正确的Next节点。 代码 代码仓库： https://github.com/lzx2005/leetcode-solution/blob/master/0117-populating-next-right-pointers-in-each-node-ii/PopulatingNextRightPointersInEachNodeIi.java 123456789101112131415161718192021public Node connect(Node root) &#123; if (root == null) return null; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty()) &#123; int size = queue.size(); while (size-- &gt; 0) &#123; Node node = queue.remove(); if (size &gt; 0) &#123; node.next = queue.peek(); &#125; if (node.left != null) &#123; queue.add(node.left); &#125; if (node.right != null) &#123; queue.add(node.right); &#125; &#125; &#125; return root;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://lzx2005.github.io/categories/算法/"},{"name":"Leetcode","slug":"算法/Leetcode","permalink":"https://lzx2005.github.io/categories/算法/Leetcode/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"算法","slug":"算法","permalink":"https://lzx2005.github.io/tags/算法/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://lzx2005.github.io/tags/Leetcode/"}]},{"title":"如何设计一个短链接网站","slug":"如何设计一个短链接网站","date":"2019-01-03T01:41:51.000Z","updated":"2019-09-10T07:28:52.435Z","comments":true,"path":"2019/01/03/如何设计一个短链接网站/","link":"","permalink":"https://lzx2005.github.io/2019/01/03/如何设计一个短链接网站/","excerpt":"","text":"在日常生活中，我们经常需要用到短链接、短网址服务，将长链接映射为短链接，更方便该路径在互联网，短信，海报等载体中传播，在运营和销售的场景下，还可以帮助工作人员统计业绩，用处非常大。 市面上比较常见的短链接有：新浪的t.cn、淘宝的tb.am、0x3.me（数据统计报表）。作为一个数据控，我更喜欢0x3.me这样的提供一整套数据报表的短网址服务，更为专业，短链接系统的价值也体现越大。 原理 短链接的保存可以采用62进制法，在数据库中，使用 十进制 的ID保存每一条长链接，例如： ID(十进制) URL ID的62进制 1 https://baidu.com 000001 2 https://jd.com?a=1&amp;b=c 000002 3 https://taobao.com/ccc/ddd/eee 000003 ... ... ... 9326778807 https://qq.com abcdef 我们将十进制的ID转换为62进制，即可得到一个短链接的Code。一个六位的Code由0-9a-zA-Z组成，总共的组合超过500亿条，显然可以满足大部分的短链接需求。 假设我们的短链接网址的域名是x.com，那么我们的短链接地址就是x.com/abcdef，我们写好后台程序，当请求进来时，我们从数据库中找到对应的长链接，使用301或者302跳转，将长链接提供给访问者，即可实现一个短链接跳转的功能。 设计 基本的原理搞清楚了，我们开始设计短链接系统，想要设计一个高性能的短链接系统，首先要确定该系统的架构，这次我们选用了Java来开发后端，MySQL作存储，前端使用Vue来进行前后端分离。当前各大企业最流行的开发的结构。在日志收集和分析的模块，我们使用ELK架构，分析每一次访问的数据，包括IP，访问地域，访问操作系统等信息，存储在ElasticSearch中，在数据和图标展示模块，我们使用ES的聚合函数进行数据聚合，并使用Echarts进行展示。 架构图 根据上述设计，我们可以画出一个短链接系统的架构图： 架构设计好后我们开始进行编码 表结构设计 首先设计短链接存储的表，这是本系统的核心表，存储着所有的短链接： 名称 类型 非空 默认值 说明 id bigint(20) unsigned True 自增主键 long_url text True 长链接 code varchar(10) True 短链接码（ID的62进制） description varchar(255) False 短链接的说明（可选） created_time timestamp False CURRENT_TIMESTAMP 创建时间 created_by varchar(32) False 创建人 del_flag tinyint(1) unsigned True 0 是否被删除 view int(10) False 0 访问次数 expired_at timestamp False 0 过期的时间（可以设置短链接过期） max_view int(10) False 0 最大访问量（可以设置短链接最大的访问量） password varchar(32) False null 可以设置密码（设置后访问需要使用密码） 在访问日志方面，我们设计的存储方式是存储至ES，使用Logstash进行日志采集： 一个典型的JSON格式访问日志应该是这样的 12345678910111213141516171819&#123; \"id\": 1469872, \"reqIp\": \"61.129.6.159\", \"reqTime\": \"2019-01-03T15:16:30.672+0800\", \"reqIsp\": \"电信\", \"reqCity\": \"上海市\", \"reqRegion\": \"上海\", \"reqCountry\": \"中国\", \"reqShortCode\": \"a0009B\", \"reqShortId\": 9161328915, \"reqStatus\": 0, \"reqMessage\": \"访问正常\", \"reqOs\": \"Windows 7\", \"reqBrowser\": \"Chrome内核\", \"reqDevice\": \"Windows\", \"reqUa\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\", \"reqUrl\": \"http://localhost:8088/a0009B\", \"reqParams\": null&#125; 根据IP计算出访问地域我们使用的是 ip2region 组件，该组件的优势是免费且速度快，缺点是需要定时更新数据库。 后端 后端我们使用目前最流行的Spring Boot + Mybatis来进行开发。主要涵盖的内容有这几块 用户管理（注册、登录） 短链接的管理（生成、删除、查看） 短链接的查看（访问日志，数据统计） 短链接的统计（每日访问统计，每次访问的数据分析） 响应短链接跳转 当短链接请求过来的时候，我们需要处理短链接跳转： 12345678910111213141516171819202122232425@GetMapping(\"/&#123;shortUrlCode&#125;\") //响应类似 x.com/abcdef 这样的请求public String redirect(@PathVariable(required = false) String shortUrlCode, HttpServletRequest request, HttpServletResponse response) throws IOException &#123; if (StringUtils.isBlank(shortUrlCode) || shortUrlCode.length() &gt; 10) &#123; return \"index\"; // 如果没有传Code，则跳转到首页去 &#125; String ipAddress = NetworkUtil.getIpAddress(request); //获得IP地址 ServiceResult serviceResult = shortUrlService.getLongUrlByShortUrl(shortUrlCode); //获得长链接信息 AccessRecordEvent accessRecordEvent = new AccessRecordEvent(); accessRecordEvent.setReqShortCode(shortUrlCode) .setServiceResult(serviceResult) .setIpAddress(ipAddress); if (null != request.getHeader(\"User-Agent\")) &#123; accessRecordEvent.setUserAgent(request.getHeader(\"User-Agent\")); //得到User-Agent以便分析浏览器、操作系统信息 &#125; Dew.cluster.mq.request(MQTopic.ACCESS_RECORD, $.json.toJsonString(accessRecordEvent)); //发送MQ请求，异步记录访问日志 if (serviceResult.getCode() == 200) &#123; Map data = (Map) serviceResult.getData(); response.sendRedirect((String) data.get(\"longUrl\")); // redirect 至长链接 return null; &#125; else &#123; // 如果找不到指定短链接Code，则跳转到404页面。 response.sendRedirect(\"/404.html\"); return null; &#125;&#125; 该方法主要实现了两件事： 记录访问日志 跳转至长链接 这也是短链接跳转的核心方法，需要对这个接口进行压力测试。 为了方便说明，简化了 最大访问量检测，过期检测以及密码检测 前端 前端我们使用 Vue+iView 进行开发，使用这个组合可以快速且优雅地开发出美观的前端应用，前端主要功能： 首页（创建短连接、用户登录、系统介绍） 短链接管理（删除、查看短链接） 访问日志（查看短链接访问日志） 图表控制台（使用Echarts对数据进行可视化展示） 效果 总结 在开发好一个短链接系统后，还需要进行大量的测试，包括单元测试，黑盒白盒，压力测试。短链接系统虽然从原理上讲并不算复杂，但是要设计和开发好一个好用且功能齐全的短链接，也是需要费一番功夫的。 同时欢迎大家访问我司短链接系统： r2a.cn 今天的文章就到这里，我们下次再见。","categories":[{"name":"思路","slug":"思路","permalink":"https://lzx2005.github.io/categories/思路/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"设计","slug":"设计","permalink":"https://lzx2005.github.io/tags/设计/"},{"name":"思路","slug":"思路","permalink":"https://lzx2005.github.io/tags/思路/"}]},{"title":"使用parallelStream并行处理集合","slug":"使用parallelStream并行处理集合","date":"2018-12-27T05:39:22.000Z","updated":"2019-09-10T07:28:52.434Z","comments":true,"path":"2018/12/27/使用parallelStream并行处理集合/","link":"","permalink":"https://lzx2005.github.io/2018/12/27/使用parallelStream并行处理集合/","excerpt":"","text":"最近在公司的风控系统搬砖，写代码时，其中的某一个步骤是将一个大的Map（内有上万条Key，Value值）遍历一遍，逐一分析每一个Key，Value值，并进行因子解析，以便建立用户画像。其中因子解析的步骤，可能会很长，涉及到数值计算，数据库查询或者接口调用，处理速度从1ms-100ms不等，当总数据量超过一万条时，对整个系统的性能损耗非常大。 使用parallelStream 我接手到的代码是这样的： 1234map.forEach((key, value) -&gt; &#123; // 处理数据，耗时1ms-100ms或更高 System.out.println(key +\":\"+ value);&#125;); 既然用到了Stream来处理，于是我便使用了parallelStream来实现集合的并行处理，只需要对Stream调用链加上parallelStream()方法即可打开： 1234map.entrySet().parallelStream().forEach(entry -&gt; &#123; // do something System.out.println(entry.getKey() +\":\"+ entry.getValue());&#125;); 该方法即可打开Java并行处理集合的功能，让我们来写方法验证该方法是否可以真的提高处理速度。 首先我们构建一个测试数据，一个只有大小为10的HashMap： 1234567private static final HashMap&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();static &#123; for(int i=0;i&lt;10;i++)&#123; map.put(\"k\"+i, i); &#125;&#125; 编写两个方法（不用并行与使用并行），都输出值，并计算耗时： 1234567891011121314151617long start = System.currentTimeMillis();map.forEach((key, value) -&gt; &#123; sleep(100); // 模拟处理时间 System.out.print(value + \",\");&#125;);long mid = System.currentTimeMillis();System.out.println(mid - start + \"ms\");map.entrySet().parallelStream().forEach(entry -&gt; &#123; sleep(100); // 模拟处理时间 System.out.print(entry.getValue() + \",\");&#125;);long end = System.currentTimeMillis();System.out.println(end - mid + \"ms\"); 执行后，得到的结果为： 120,1,2,3,4,5,6,7,8,9,1129ms9,0,5,1,7,3,6,2,4,8,321ms 从以上的输出可以得出的基本结论有： 使用parallelStream的代码确实是并行运行了，因为输出不是正序的 使用parallelStream确实可以在某种程度提高集合处理速度 线程安全 显而易见，parallelStream是非线程安全的，举个简单的例子： 123456789101112131415161718192021222324252627private static List&lt;Integer&gt; list1 = new ArrayList&lt;&gt;();private static List&lt;Integer&gt; list2 = new ArrayList&lt;&gt;();private static List&lt;Integer&gt; list3 = new ArrayList&lt;&gt;();private static Lock lock = new ReentrantLock();public static void main(String[] args) &#123; IntStream.range(0, 10000).forEach(list1::add); IntStream.range(0, 10000).parallel().forEach(list2::add); IntStream.range(0, 10000).forEach(i -&gt; &#123; lock.lock(); try &#123; list3.add(i); &#125;finally &#123; lock.unlock(); &#125; &#125;); System.out.println(\"串行执行的大小：\" + list1.size()); System.out.println(\"并行执行的大小：\" + list2.size()); System.out.println(\"加锁并行执行的大小：\" + list3.size());&#125;串行执行的大小：10000并行执行的大小：9595加锁并行执行的大小：10000 所以，我们在使用parallelStream时，需要注意线程安全的问题，该加锁的就加锁，外部调用的ArrayList，HashMap等也必须使用和其对等的线程安全类，例如：ConcurrentHashMap等。","categories":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lzx2005.github.io/tags/Java/"},{"name":"Stream","slug":"Stream","permalink":"https://lzx2005.github.io/tags/Stream/"}]},{"title":"《深度学习入门：基于Python的理论与实现》读书笔记（二）感知机","slug":"deep-learning-read-perceptron","date":"2018-12-04T08:42:03.000Z","updated":"2019-09-10T07:28:52.432Z","comments":true,"path":"2018/12/04/deep-learning-read-perceptron/","link":"","permalink":"https://lzx2005.github.io/2018/12/04/deep-learning-read-perceptron/","excerpt":"","text":"今天学习了第二章：感知机。 感知机是什么 感知机(perceptron)这一算法是由美国学者 Frank Rosenblatt 在 1957 年提出来的。感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送 电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电 流不同的是，感知机的信号只有“流 / 不流”(1/0)两种取值。在本书中，0 对应“不传递信号”，1 对应“传递信号”。 x1、x2 是输入信号， y是输出信号，w1、w2 是权重(w是weight的首字母)。图中的○称为“神 经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重(w1x1、w2x2)。神经元会计算传送过来的信号的总和，只有当这个总和超过 了某个界限值时，才会输出1。这也称为“神经元被激活”。这里将这个界 限值称为阈值，用符号 θ 表示。 把上述内容用数学式来表示，就是式(2.1)： \\[ y=\\begin{cases} 0 &amp; (w_1x_1+w_2x_2) \\leqslant \\theta\\\\ 1 &amp; (w_1x_1+w_2x_2) &gt; \\theta \\end{cases}\\tag{2.1}\\label{l2.1} \\] 感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个 信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。 感知机实现与门电路 与门 方案 选择方法有无数多个。比如，当(w1, w2, θ) = (0.5, 0.5, 0.7) 时，可 以 满 足 图 2-2 的 条 件。此 外，当 (w1, w2, θ)为 (0.5, 0.5, 0.8) 或者 (1.0, 1.0, 1.0) 时，同样也满足与门的条件。设定这样的参数后，仅当 x1 和 x2 同时为 1 时，信号的加权总和才会超过给定的阈值 θ。 实现 1234567def AND(x1, x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp &lt;= theta: return 0 elif tmp &gt; theta: return 1 结果： 1234AND(0, 0) # 输出0 AND(1, 0) # 输出0 AND(0, 1) # 输出0 AND(1, 1) # 输出1 导入权重和偏置 式(2.1)的θ换成−b，于是就可以用式(2.2)来表示感知机的行为： \\[ y=\\begin{cases} 0 &amp; (b+w_1x_1+w_2x_2) \\leqslant 0\\\\ 1 &amp; (b+w_1x_1+w_2x_2) &gt; 0 \\end{cases}\\tag{2.2}\\label{l2.2} \\] 此处，b 称为偏置，w1 和 w2 称为权重。如式(2.2)所示，感知机会计算输入信号和权重的乘积，然后加上偏置，如果这个值大于 0 则输出 1，否则输出 0。下面，我们使用 NumPy，按式(2.2)的方式实现感知机： 123456789def AND(x1, x2): x = np.array([x1, x2]) # 输入信号 w = np.array([0.5, 0.5]) # 权重 b = -0.7 # 偏置 tmp = np.sum(w*x) + b # 计算 if tmp &lt;= 0: return 0 else: return 1 这里把 −θ 命名为偏置 b，但是请注意，偏置和权重 w1、w2 的作用是不一样的。具体地说，w1 和 w2 是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度(输出信号为 1 的程度)的参数。比如，若 b 为−0.1，则只要输入信号的加权总和超过 0.1，神经元就会被激活。但是如果 b为 −20.0，则输入信号的加权总和必须超过 20.0，神经元才会被激活。像这样，偏置的值决定了神经元被激活的容易程度。另外，这里我们将 w1 和 w2 称为权重，将 b 称为偏置，但是根据上下文，有时也会将 b、w1、w2 这些参数统称为权重。 实现与非门和或门 与非门 123456789def NAND(x1, x2): x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) # 仅权重和偏置与AND不同! b = 0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1 或门 123456789def OR(X1, X2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.2 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else return 1 感知机的局限性 因为感知机是线性空间，所以单层感知机无法实现异或门(XOR gate)。 线性和非线性 由图 2-8 这样的曲线分割而成的空间称为非线性空间，由直线分割而成的空间称为线性空间。 多层感知机 我们可以使用如下结构来实现异或门： 异或门是一种多层结构的神经网络。这里，将最左边的 一列称为第 0 层，中间的一列称为第 1 层，最右边的一列称为第 2 层。图所示的感知机与前面介绍的与门、或门的感知机(图 2-1)形状不同。实际上，与门、或门是单层感知机，而异或门是 2 层感知机。叠加了多层的感知机也称为多层感知机(multi-layered perceptron)。 这种 2 层感知机的运行过程可以比作流水线的组装作业。第 1 段(第 1 层) 的工人对传送过来的零件进行加工，完成后再传送给第 2 段(第 2 层)的工人。 第 2 层的工人对第 1 层的工人传过来的零件进行加工，完成这个零件后出货 (输出)。 像这样，在异或门的感知机中，工人之间不断进行零件的传送。通过这样的结构 ( 2层结构 )， 感知机得以实现异或门 。 这可以解释为 “ 单层感知机无法表示的东西，通过增加一层就可以解决”。也就是说，通过叠加层(加深 层)，感知机能进行更加灵活的表示。 小结 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。 感知机将权重和偏置设定为参数。 使用感知机可以表示与门和或门等逻辑电路。 异或门无法通过单层感知机来表示。 使用2层感知机可以表示异或门。 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。 多层感知机(在理论上)可以表示计算机。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/categories/读书笔记/"},{"name":"深度学习","slug":"读书笔记/深度学习","permalink":"https://lzx2005.github.io/categories/读书笔记/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://lzx2005.github.io/tags/深度学习/"},{"name":"Python","slug":"Python","permalink":"https://lzx2005.github.io/tags/Python/"},{"name":"读后感","slug":"读后感","permalink":"https://lzx2005.github.io/tags/读后感/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/tags/读书笔记/"}]},{"title":"《深度学习入门：基于Python的理论与实现》读书笔记（一）","slug":"deap-learning-read-python-basic","date":"2018-12-03T14:52:23.000Z","updated":"2019-09-10T07:28:52.431Z","comments":true,"path":"2018/12/03/deap-learning-read-python-basic/","link":"","permalink":"https://lzx2005.github.io/2018/12/03/deap-learning-read-python-basic/","excerpt":"","text":"今天阅读的是《深度学习入门：基于Python的理论与实现》第0-19页部分，主要介绍了Python的知识和相应的库的使用。 Python基础知识 Python Python 是一个简单、易读、易记的编程语言，而且是开源的，可以免费地自由使用。Python 可以用类似英语的语法编写程序，编译起来也不费力，因此我们可以很轻松地使用 Python。 版本的区别 Python有Python 2.x和Python 3.x两个版本。因此，在安装 Python 时，需要慎重选择安装 Python 的哪个版本。这是因为两个版本之间没有向后兼容性。 NumPy和Matplotlib库 NumPy 是用于数值计算的库，提供了很多高级的数学算法和便利的数组(矩阵)操作方法。本书中将使用这些便利的方法来有效地促进深度学习的实现。 Matplotlib 是用来画图的库。使用 Matplotlib 能将实验结果可视化，并在视觉上确认深度学习运行期间的数据。 Python的使用 查看Python版本 安装好Python3后，打开终端，输入python3 --version即可查询到当前安装的python版本： 12&gt; python3 --versionPython 3.7.1 在官网安装的Python2.7和Python3.7在终端访问的方式略有不同： Python2 : python --version Python3 : python3 --version (Windows环境为py --version) Python 解释器 Python 解释器也被称为“对话模式”，用户能够以和 Python 对话的方式进行编程。比如，当用户询问“1 + 2 等于几?”的时候，Python 解释器会回答“3”，所谓对话模式，就是指这样的交互： 1234567&gt; python3Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 03:13:28)[Clang 6.0 (clang-600.0.57)] on darwinType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; 1+23&gt;&gt;&gt; 算数计算 *表示乘法，/表示除法，**表示乘方(3**2是3的2次方)。这部分跟大部分的编程语言类似，但是也使用了**这样的在Java中没见过的语法糖 1234567891011&gt;&gt;&gt; 1+23&gt;&gt;&gt; 1-2-1&gt;&gt;&gt; 4*520&gt;&gt;&gt; 7/51.4&gt;&gt;&gt; 3**29&gt;&gt;&gt; 数据类型 Python编程中有数据类型(data type)这一概念。数据类型表示数据的性质，有整数、小数、字符串等类型。Python 中的 type() 函数可以用来查看数据 类型。 1234567&gt;&gt;&gt; type(10)&lt;class 'int'&gt;&gt;&gt;&gt; type(2.718)&lt;class 'float'&gt;&gt;&gt;&gt; type('abc')&lt;class 'str'&gt;&gt;&gt;&gt; 变量 可以使用 x 或 y 等字母定义变量(variable)。此外，可以使用变量进行计算，也可以对变量赋值。 &gt; 注：python2.7的打印方式为 print x 1234&gt;&gt;&gt; x=10&gt;&gt;&gt; print(x)10&gt;&gt;&gt; Python 是属于“动态类型语言”的编程语言，所谓动态，是指变量的类型是根据情况自动决定的。在上面的例子中，用户并没有明确指出“x 的类型是 int(整型)”，是 Python 根据 x 被初始化为 10，从而判断出 x 的类型为int 的。此外，我们也可以看到，整数和小数相乘的结果是小数(数据类型的自动转换)。 另外，“#”是注释的意思，它后面的文字会被 Python 忽略。 列表 创建和打印列表 1234&gt;&gt;&gt; a = [1, 2, 3, 4, 99] # 生成列表&gt;&gt;&gt; print(a)[1, 2, 3, 4, 99]&gt;&gt;&gt; 切片访问 Python 的列表提供了切片 (slicing)这一便捷的标记法。使用切片不仅可以访问某个值，还可以访问列表的子列表(部分列表)。 进行列表的切片时，需要写成 a[0:2] 这样的形式。a[0:2] 用于取出从索引为 0 的元素到索引为 2 的元素的前一个元素之间的元素。另外，索引 −1 对应最后一个元素，−2 对应最后一个元素的前一个元素。 1234567&gt;&gt;&gt; a[0:2] # 获取索引为0到2(不包括2!)的元素 [1, 2]&gt;&gt;&gt; a[1:] # 获取从索引为 1 的元素到最后一个元素 [2, 3, 4, 99]&gt;&gt;&gt; a[:3] # 获取从第一个元素到索引为 3(不包括 3 !)的元素[1, 2, 3]&gt;&gt;&gt; a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素 [1, 2, 3, 4]&gt;&gt;&gt; a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素 [1, 2, 3]&gt;&gt;&gt; 字典 Python字典类似Java中的Map结构，也就是Json格式中的Key:value结构： 12345&gt;&gt;&gt; me = &#123;'height':180&#125; # 生成字典&gt;&gt;&gt; me['height'] # 访问元素 180&gt;&gt;&gt; me['weight'] = 70 # 添加新元素&gt;&gt;&gt; print(me)&#123;'height': 180, 'weight': 70&#125; 布尔型 布尔型则比较好理解，类似Java中的Boolean类型，但是Python中的判断更加像平时说话的方式，用and替代&amp;&amp;，用or替代||，使代码可读性大大加强： 12345678&gt;&gt;&gt; hungry = True # 饿了?&gt;&gt;&gt; sleepy = False # 困了?&gt;&gt;&gt; type(hungry)&lt;class 'bool'&gt;&gt;&gt;&gt; not hungryFalse&gt;&gt;&gt; hungry and sleepy # 饿并且困 False&gt;&gt;&gt; hungry or sleepy # 饿或者困 True If 语句 Python中的空白字符具有重要的意义。上面的if语句中，if hungry:下面的语句开头有4个空白字符。它是缩进的意思，表示当前面的条件(if hungry)成立时，此处的代码会被执行。这个缩进也可以用 tab 表示，Python 中推荐使用空白字符。 也就是说，Python使用缩进代替了Java中的{、}来对代码进行分块。同时强烈建议使用四空格代替\\t来进行缩进，这样在Win平台和Linux平台显示的缩进就会相同，而不会出现格式乱掉的情况。 123456&gt;&gt;&gt; hungry = True&gt;&gt;&gt; if hungry:... print(\"I'm hungry\")...I'm hungry&gt;&gt;&gt; For 语句 for...in...更加符合平时的阅读习惯： 1234567&gt;&gt;&gt; for i in [1, 2, 3]:... print(i)...123&gt;&gt;&gt; 函数 可以将一连串的处理定义成函数(function)： 123456&gt;&gt;&gt; def hello():... print(\"hello world\")...&gt;&gt;&gt; hello()hello world&gt;&gt;&gt; 由于是动态类型语言，所以不需要指定返回值类型。 退出对话模式 关闭Python解释器时，Linux或Mac OS X的情况下输入Ctrl-D(按住Ctrl，再按 D 键);Windows 的情况下输入 Ctrl-Z，然后按 Enter 键。当然我更喜欢在对话框中输入exit()来退出： 12&gt;&gt;&gt; exit()&gt; Python 脚本文件 可以将Python 程序保存为文件，然后(集中地)运行这个文件，文件后续名为.py。 使用方式为在终端中输入 python3 filename.py 即可执行Python脚本文件，非常方便。 类 Python 中使用 class 关键字来定义类，类要遵循下述格式(模板)： 1234567class 类名: def __init__(self, 参数, ...): # 构造函数 ... def 方法名1(self, 参数, ...): # 方法1 ... def 方法名2(self, 参数, ...): # 方法2 ... 这里有一个特殊的 init 方法，这是进行初始化的方法，也称为构造函数(constructor), 只在生成类的实例时被调用一次。此外，在方法的第一个参数中明确地写入表示自身(自身的实例)的 self 是 Python 的一个特点。 NumPy 在深度学习的实现中，经常出现数组和矩阵的计算。NumPy 的数组类 (numpy.array)中提供了很多便捷的方法 使用Numpy 基本运算 12345&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; x = np.array([1.0, 2.0, 3.0]) &gt;&gt;&gt; print(x)[ 1. 2. 3.]&gt;&gt;&gt; type(x)&lt;class 'numpy.ndarray'&gt; 如果出现ModuleNotFoundError: No module named 'numpy'则需要使用pip安装Numpy 12&gt; pip3 install numpy&gt; 安装完成后即可使用 NumPy 数组的算术运算： 1234&gt;&gt;&gt; y = np.array([2.0, 3.0, 4.0])&gt;&gt;&gt; x+yarray([3., 5., 7.])&gt;&gt;&gt; 如果元素个数不同，程序就会报错，所以元素个数保持一致非常重要。“对应元素的”的英文是 element-wise，比如“对应元素的乘法”就是element-wise product。 生成多维数组 1&gt;&gt;&gt; A = np.array([[1, 2], [3, 4]]) 矩阵运算 1234567&gt;&gt;&gt; B = np.array([[3, 0],[0, 6]])&gt;&gt;&gt; A + Barray([[ 4, 2], [ 3, 10]])&gt;&gt;&gt; A * 10array([[ 10, 20], [ 30, 40]]) 广播 NumPy 中，形状不同的数组之间也可以进行运算。之前的例子中，在2×2 的矩阵 A 和标量 10 之间进行了乘法运算。在这个过程中，如图 1-1 所示，标量 10 被扩展成了 2 × 2 的形状，然后再与矩阵 A 进行乘法运算。这个巧妙的功能称为广播(broadcast)。 访问元素 1234&gt;&gt;&gt; X[0][1] # (0,1)的元素或者&gt;&gt;&gt; for row in X:... print(row) Matplotlib 在深度学习的实验中，图形的绘制和数据的可视化非常重要。Matplotlib 是用于绘制图形的库，使用 Matplotlib 可以轻松地绘制图形和实现数据的可视化。 绘制简单图形 1234567import numpy as npimport matplotlib.pyplot as plt# 生成数据x = np.arange(0, 6, 0.1) # 以0.1为单位，生成0到6的数据y = np.sin(x)plt.plot(x, y)plt.show() 这里使用NumPy的arange方法生成了[0, 0.1, 0.2, ..., 5.8, 5.9]的数据，将其设为 x。对 x 的各个元素，应用 NumPy 的 sin 函数 np.sin()，将 x、y 的数据传给 plt.plot 方法，然后绘制图形。最后，通过 plt.show() 显示图形。运行上述代码后，就会显示图 1-3 所示的图形： 显示图像 pyplot中还提供了用于显示图像的方法imshow()。另外，可以使用matplotlib.image 模块的 imread() 方法读入图像。 1234567import matplotlib.pyplot as pltfrom matplotlib.image import imreadimg = imread('lena.png') # 读入图像(设定合适的路径!)plt.imshow(img)plt.show()","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/categories/读书笔记/"},{"name":"深度学习","slug":"读书笔记/深度学习","permalink":"https://lzx2005.github.io/categories/读书笔记/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://lzx2005.github.io/tags/深度学习/"},{"name":"Python","slug":"Python","permalink":"https://lzx2005.github.io/tags/Python/"},{"name":"读后感","slug":"读后感","permalink":"https://lzx2005.github.io/tags/读后感/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://lzx2005.github.io/tags/读书笔记/"}]},{"title":"使用MongoDB的位置索引实现“附近的人”的计算","slug":"mongo-point","date":"2017-04-27T23:57:41.000Z","updated":"2019-09-10T07:28:52.434Z","comments":true,"path":"2017/04/28/mongo-point/","link":"","permalink":"https://lzx2005.github.io/2017/04/28/mongo-point/","excerpt":"","text":"使用MongoDB的位置索引实现“附近的人”的计算 最近一直在忙工作和毕设，没什么时间写博客，真的非常不好意思，在写毕设的时候，有一个需求，就是找到地图上附近的点，我使用的是MongoDB存储我的信息，正好Mongo也有位置索引，也可以进行位置搜索，而且使用起来非常方便，这边记录一下。 语言：Java8 框架：Spring MVC、Spring、Mybatis 数据库：MongoDB 3.4.2 数据保存 你需要将坐标数据以特定的格式保存到MongoDB，我的数据源如下所示： 123456&#123; \"restaurantId\" : \"fd433baf3a7e4722bdd2c7a9c1a0f7c0\", \"restaurantName\" : \"XX餐厅\", \"belong\" : 2, \"createTime\" : \"2017-04-26T09:24:43.266Z\"&#125; 想要给他加上一个位置信息、官方文档上是这样写的： 123loc : [ &lt;longitude&gt; , &lt;latitude&gt; ]和loc : &#123; lng : &lt;longitude&gt; , lat : &lt;latitude&gt; &#125; 用这样两种方式都是可以的，记住，一定是lng在前，lat在后，之前写的时候就是因为顺序写反了，导致我一直算不出附近的点，非常气。 于是我的数据源就变成了如下 12345678910&#123; \"restaurantId\" : \"fd433baf3a7e4722bdd2c7a9c1a0f7c0\", \"restaurantName\" : \"XX餐厅\", \"position\" : [ 120.576861, 30.000107 ], \"belong\" : 2, \"createTime\" : \"2017-04-26T09:24:43.266Z\"&#125; 在MongoDB的客户端选择上，Spring Data的MongoTemplate非常方便，可以使用它对Mongo做一系列操作， 将数据源插入MongoDB以及对位置信息做索引的代码如下： 1234567//插入数据，我使用了Restaurant这个类作为Entity，其实什么数据都可以，关键就是要有正确格式的坐标数据mongoTemplate.insert(restaurant,collectionName);//定义某个key为位置索引，索引格式为GEO_2DGeospatialIndex position = new GeospatialIndex(\"position\");position.typed(GeoSpatialIndexType.GEO_2D);//索引操作，给position这个key添加位置索引mongoTemplate.indexOps(collectionName).ensureIndex(position); 这样您保存的数据就已经做好了索引，我们可以多添加一些点。 数据查询 假设你有一个APP，你获取到了你当前的坐标，想要获取左边周围固定范围的点的集合，这时候可以使用MongoDB来做，因为在上面我们已经对position做了索引，所以可以使用mongoTemplate的NearQuery类进行附近的点的查询，查询的代码如下： 1234567891011//得到当前用户的坐标double lng = 120.576861;double lat = 30.000107;//定义范围，在这个范围内的点都会被返回double length = 1000.0;//NearQuery专门为位置查询而生，Metrics类储存了单位的数据，让您可以设置不同的单位，这里我们设置为KILOMETERS(千米)NearQuery nearQuery = NearQuery.near(lng, lat, Metrics.KILOMETERS).maxDistance(length);//查询成功后则返回GeoResults&lt;T&gt;，该类专门存储位置信息和Entity信息，包括距离等有用的信息。GeoResults&lt;Restaurant&gt; geoResults = mongoTemplate.geoNear(nearQuery, Restaurant.class, collectionName); 让我们来看一下GeoResults内部的格式是怎么样的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; \"averageDistance\": &#123; \"metric\": \"KILOMETERS\", \"normalizedValue\": 0.003964060840443525, \"unit\": \"km\", \"value\": 25.28332311668394 &#125;, \"content\": [ &#123; \"content\": &#123; \"belong\": 2, \"createTime\": 1493198674010, \"position\": [ 120.152171, 30.266635 ], \"restaurantId\": \"eea5e08be3dc4341b66ec0fc27b9a085\", \"restaurantName\": \"A\" &#125;, \"distance\": &#123; \"metric\": \"KILOMETERS\", \"normalizedValue\": 0.000014026592741484346, \"unit\": \"km\", \"value\": 0.08946353014839274 &#125; &#125;, &#123; \"content\": &#123; \"belong\": 2, \"createTime\": 1493198683266, \"position\": [ 120.576861, 30.000107 ], \"restaurantId\": \"fd433baf3a7e4722bdd2c7a9c1a0f7c0\", \"restaurantName\": \"B\" &#125;, \"distance\": &#123; \"metric\": \"KILOMETERS\", \"normalizedValue\": 0.007914095088145565, \"unit\": \"km\", \"value\": 50.47718270321948 &#125; &#125; ]&#125; 我们发现MongoTemplate为我们封装了很多有用的信息，包括averageDistance平均距离都有。这样我们就可以把附近的点的信息找到啦，是不是非常方便？ 反馈与建议 微博：[@lzx2005](http://weibo.com/u/2557929062) 邮箱：crow2005@vip.qq.com ​","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://lzx2005.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://lzx2005.github.io/tags/MongoDB/"},{"name":"LBS","slug":"LBS","permalink":"https://lzx2005.github.io/tags/LBS/"}]}]}